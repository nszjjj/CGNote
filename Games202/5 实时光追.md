# 前情提要

先前的 lecture 提到了 Real-Time Physically-Based Materials

- Microfacet BRDF, NDF, shadowing-masking（但是依旧只能处理直接光照）

- Kulla-Conty Approximation for multiple bounces

- Disney principled BRDF（Artist Friendly）

提到了使用 Linearly Transformed Cosines (LTCs) 对 polygonal lighting进行高效的近似

提到了为什么202不讲Volumetric/scattering materials：因为他们依赖太多前置的知识了，例如RTE、BSSRDF、single/multiple sacttering等

实时光追的突破源自于硬件的突破

# Basic of Denoising

## 1 sample per pixel：Cause of Noise

简称1 SPP，1 SPP意味着：

- 1 rasterization（即相机到像素的primary ray）

- 1 ray（像素到光源的visibility）

- 1 ray（secondary bounce）

- 1 ray（光线弹射一次后的 可见性，secondary visibility）

前两个加起来是直接光照。相机到像素本质是光栅化，所以用rasterization代替了ray

现在的硬件支持实时 SPP 的次数不多，因此只讨论最基础的1 SPP的情况。但是1 SPP 的效果非常noisy，所以<mark>RTRT的核心就是Denoising</mark>。

在1 SPP下denoising的目标是这样的：

1. 质量（no overblur，no artifacts，keep all details）

2. Speed（<2ms to denoise one frame）

overblur：过于夸张的Filter会去掉噪点，也会去掉有用的信息

artifacts：渲染过程中出现的可见的瑕疵

这样的要求导致原先的滤波手段不太符合要求，例如：

- Sheared filtering series（SF, AAF, FSF, MAAF）

- Other offline filtering methods（IPP, BM3D, APR）

- Deep learning series（CNN, Autoencoder）

降噪的方法很多，但是针对RTRT的很少。或多或少在采样或者耗时上都不满足。

---

工业界的做法的核心思想是使用Temporal Denoising，在时间上处理：

1. 在滤波当前帧的情况下永远认为前一帧是已经滤波好了的

2. 使用motion vector寻找shading point在上一帧的位置

3. 复用上一帧的降噪信息

这是一种Inspired by TAA的方法

在这种方法中，通常需要 G-Buffer 辅助

geometry buffer 能够记录渲染过程中可以获得的信息，这些信息通常是 screen space 的，内容与片元密切相关。其内容通常源自渲染过程中用到或者产生的数据，例如片元的世界坐标、pixel depth

## Motion Vector: The Basis of Denoising

> 这块的东西我确信 202 讲的不够清晰

首先 Velocity Buffer$ ^{[1]}$ 和 Motion Vector Buffer 本质上是同一个概念，它们的作用都是存储每个像素或顶点在连续帧之间的运动信息（即运动向量，motion vector）。

使用 motion vector 的核心目的是为了追踪世界坐标在屏幕空间中的运动，而不是直接与某个具体的物体绑定。它的本质是通过世界坐标的连续性，将当前帧的像素与上一帧的像素关联起来，从而利用时间域的信息（即上一帧的降噪结果）来改善当前帧的质量。

Motion Vector是一个屏幕空间的二维向量，定义如下：

```glsl
motion_vector = current_screen_pos.xy - prev_screen_pos.xy
```

但是这里有个问题：像素展示的是场景中的物体的某一块区域$\Delta$的颜色，这块区域有个世界空间坐标$s$。那么前一帧如果物体发生移动，导致$\Delta$并不在$s$位置，此时该取用谁来确定前一帧像素的屏幕空间位置？因此要说明 Motion Vector 的种类。

Motion Vector 分为两种：Camera Motion Vector 和 Object Motion Vector

仅给定运动矢量纹理无法推断$^{[2]}$片元在屏幕上的运动是否仅由相机运动、物体运动还是两者的结合引起。

- Camera Motion Vector 的计算与场景复杂度无关，仅需两帧的相机矩阵即可。

- Object Motion Vector 的计算与场景复杂度有关，需要考虑每个物体的运动，同时也需要应用相机的运动贡献

---

根据相关信息推导出 Motion Vector 的操作叫做 Back Projection。

> 核心是确定当前帧的像素在上一帧的哪里

在这个计算的过程中，究竟计算的是上面的提到的两种 Motion Vector 中的哪一种？据观察网络博客$^{[3]}$所提供的思路，相当于是两个都计算了。本身理应是针对每个片元的计算操作，但是由于顶点着色器数据传到片元着色器会进行插值的性质，因此在顶点着色器 foreach vertex 的本质实际上就是针对每个要渲染的物体的操作，因此也把 Object Motion Vector 计算进去了。

但是1小时16分那里讲Temporal Failure的时候提到的，相机没移动但是场景物体移动了，此时他认为$s = s'$，这相当于算的是 Camera Motion Vector。

具体计算思路：

1. 确定当前像素的世界坐标$s$：一般情况下G-Buffer里会有记录片元的世界坐标，没有的话只能手动计算
2. 确定上一帧的世界坐标$s'$：如果没发生运动那$s=s'$，否则由$s'\overset{T}\rightarrow s$得到$s\overset{T^{-1}}\rightarrow s'$
3. 根据$s'$确认投影到屏幕空间的坐标$x'$（当前片元在屏幕空间的坐标是$x$）
4. $x-x'$即为 motion vector

---

闫老师课上说这样逆推世界坐标，但是我疑惑的是应该不用乘$M^{-1}$，那样就是模型坐标了。我怀疑他说的s是模型坐标不是世界坐标。

$$
s = M^{-1}V^{-1}P^{-1}E^{-1}x
$$

其中

- x是屏幕空间坐标（二维）加上z值作为z通道

- $E$是相机外参矩阵$V$的逆矩阵，用于视口变换，不过OpenGL中视口变换是自动完成的

---

不难发现确实会出现一些极端情况，导致本帧某些片元无法得到motion vector。比如说相同的世界坐标，这一帧在物体A上，但是上一帧物体 A 还没出现，该世界坐标处没有任何物体；或是说上一帧的世界坐标到相机之间有其它遮挡物。这些情况下无法直接使用时间域的去噪手段。这里的细节可以看后文章节。

---

CV中有一种手法叫做 Optical Flow（光流），Motion Vector类似于一种光流，但是比他更准确。在 Ray Tracing 里使用光流的效果和速度也并不是很符合预期。

## Temporal Accum./Denoising

有了一个片元在两帧上的信息，接下来就是利用前一帧。最简单的方式就是线性混合

做如下定义：

- $\sim$：unfiltered 未filter的，噪声较多的内容

- $-$：filtered 没有噪声或者噪声比较小的

那么在空间上的降噪公式是：

$$
\bar{C}^{(i)}=Filter[\bar{C}^{(i)}]
$$

在时间上的降噪公式是：

$$
\bar{c}^i = \alpha\bar{C}^{(i)}+(1-\alpha)\bar{C}^{(i-1)}
$$

其中$\alpha$表示平衡系数，即当前帧的贡献，一般$\alpha \in [0.1,0.2]$，这意味着上一帧通常要贡献80%~90%

要点：

- 所有降噪/滤波要保持能量守恒，并不应该使画面变亮/暗

<mark>缺陷</mark>：

- 降噪滤波可能导致AO缺失（尤指contact hardening/contact shadow）

## Temporal Failure

有一些典型的，不能够使用上一帧信息的情况;

<mark>Temporal Info is not always visible</mark>，其典型情况如下：

- burn-in period：突然切换场景（需要burn-in period来累计新场景的几帧信息以供后续复用）

- screen space issue：Zoom out（倒退着走）

- disocclusion：相机移动导致的物体突然间不被遮挡

<mark>Shading Error</mark>：有些时候 Temporal Failure 源自于 Shading 出了问题。

- 光照突然发生巨大变化
- 场景完全没动（Motion Vector为0）但是光源动了，导致 detached shadow 问题

<mark>Glossy Reflection Error</mark>：

- 宏观表现是反射的影子的变化和物体实际变化有一点时间的延迟
- 传统 Motion Vector 表示的是物体的变化，无法捕捉光影的变化

## Adjustments to Temporal Failure

- 强行用上一帧的信息：拖尾（lagging）

- 有选择地用上一帧的信息
  
  - Clamping：对本帧周围N*N计算均值$\mu$和方差$σ²$，上一帧的颜色先clamp到$\mu \pm a ·\sigma^2$，其中$a \in [1,3]$
  
  - Detection：检测要不要用前一帧的信息，如果 Motion Vector 要到另一个物体上找信息（利用 ID Map）那肯定不太适合用前一帧的。

针对检测到不能用：

- $\alpha$根据Temporal Failure的程度调节一下，可以极端到完全不用上一帧的值

- 更相信当前帧意味着<mark>重新引入一些噪声</mark>

# Filter Methods

## Spectial Denoising

如何对本帧的信息进行滤波：Cross / joint bilateral Filter 及其变体

噪声在高频或者低频都有可能出现，有用的信息也同理。高通滤波/低通滤波只能得到一个相对可以接受的结果，并且伴有有用的信息的丢失。

> 图像某点颜色本身不代表频率，颜色变化表示频率

关于图像的高低频：

- 高频分量对应于图像中快速变化的区域，例如边缘、纹理和细节

- 低频分量对应于图像中变化缓慢的区域，例如平滑的背景和大的色块

## Gaussian Filter

高斯滤波是因为使用高斯函数作为卷积核而得名。

$$
G(x, y) = A \cdot e^{-\frac{x^2 + y^2}{2\sigma^2}}
$$

二维高斯函数是二维正态的概率密度函数乘以一个系数，在系数不为1时相当于是未归一化的二维正态的概率密度函数，并且一般也不需要是归一化的。归一化的意义是在整个二维空间上积分为1。

网上说的是高斯滤波采样范围是`n*n`的正方形，但是闫老师讲的似乎是半径$\sigma$的圆形范围。

用高斯函数生成采样核的操作叫做<mark>离散化高斯核</mark>。

如果是正方形区域，就是以高斯函数中心为中心，以一定的步长采样高斯函数 N x N 的点，形成一个$N*N$的采样核的权重。步长和标准差没有直接关系，但通常情况下会让$N*N$正方形边界落在$3\sigma$处，此范围以外的像素不参与滤波。因为高斯函数覆盖范围其实是无限大的，远处的值虽然存在，但是接近0，参与贡献的意义不大。

实际应用中由于高斯函数是对称的，所以可以使用单侧的函数。会使用中心点$i$邻域里每个点到中心点的距离绝对值去查高斯函数`weight_ij = G(Distance(i, j), sigma)`。注意

由于高斯函数不是归一化的，最后操作是<u>加权的值的总和/权重总和</u>。根据高斯函数的性质，一般情况下权重总和不会出现值为零的情况。

注：像素$i$的邻域也包括$i$本身

<mark>高斯滤波的特征</mark>：

- 是低通滤波

- 线性滤波

- 可分离性：可分解为两个1D高斯滤波

## Bilateral filtering

上文提到了高斯滤波是低通滤波，并且根据已知，（图像纹理内容）边界一定是高频信息：图像颜色变化剧烈则认为是边界。

双边滤波对范围内的每个像素加权平均，其权重同时基于像素的欧几里得距离和辐射差异（例如，颜色强度、深度距离等）。其原则是显而易见的：如果像素和中心点比较，分别在边界两侧，则该像素不应当对滤波产生贡献（或贡献很小）。

空间距离权重（空间域权重）的公式如下：

$$
w_{space} = \exp(-\frac{∥p-q∥^2}{2\sigma_s^2})
$$

其中$p$中心像素的坐标；$q$是邻域像素的坐标；$∥p-q∥$是像素的欧几里得距离；$\sigma_s$是控制空间权重衰减的参数（空间标准差），值越大，平滑效果越强

像素强度差异权重（值域权重）如下：

$$
w_{range} = \exp(-\frac{(I_p-I_q)^2}{2\sigma_r^2})
$$

其中$I_p$是中心像素的强度值；$I_q$是邻域像素的强度值；$\sigma_r$是控制像素值差异权重衰减的参数（值域标准差），值越大，允许的像素值差异越大

双边滤波的的总权重是空间距离权重和像素强度差异权重的乘积：

$$
w(p,q) = w_{space}(p,q) ·w_{range}(I_p,I_q)
$$

由于$e^x ·e^y = e^{x+y}$，也可以把上面的两个式子直接写在一起。

然后会发现双边滤波的权重函数和高斯函数很相似（其实使用的就是高斯函数）

双边滤波的特点：

- 保持边缘

- 非线性

## Joint Bilateral filtering

> 亦称作Cross Bilateral filtering

由于噪声和实际内容的颜色差异可能也很大，这导致<mark>双边滤波算法分不清噪声和边界</mark>，依旧有可能保存噪声。所以有了联合双边滤波：高斯滤波只考虑一个参量（距离），双边滤波考虑两个参量（距离和颜色），那联合双边滤波可以考虑更多的参量用来指导滤波。

> 联合双边滤波很适合处理蒙特卡洛方法产生的噪声。

G-Buffer 中得到的很多信息可以指导联合双边滤波，如法线、深度、ID 等等。并且 G-Buffer 本身是无噪声的。例如两个点深度差异过大，就不要求贡献到（或尽量减少贡献到）滤波计算中

联合双边滤波的权值函数不一定是高斯，任何随着距离衰减的函数都可以，如下所示：

<img src="file:///D:/CGNote/Games202/img/lecture13_1.png" title="" alt="函数示意" data-align="center">

哪怕使用高斯，底下也不必除那个标准差，因为乘除常数并不改变衰减的性质。

因为不同的控制参量都是作为指数参与的运算（比如双边滤波就是两个$w$相乘），贡献大小全靠指数部分的分母控制。所以需要其他类型的信息指导双边滤波的时候，只需要以相同的格式乘到后面就行

联合双边滤波，这个参量选用什么（深度、法向等等），分母（那个含$\sigma$的）选多大都是自己决定的，实际使用的时候需要专门去调。当然每一项的分母可以不那么严格，但是多项组合在一起效果就还可以。

最后公式就是$w(p,q) = w_{space}(p,q) ·w_{range}(I_p,I_q) ·w_{some\_metrics}(p,q) ·...$

有几个参量（metrics）就搁后面乘几个。

## Implement Large Filter

当滤波器的滤波核大小过大的时候，例如64*64，就会导致滤波过程显著变慢。对此工业界一般有两种做法

- Separate Passes：拆分实现

- Progressively Growing Sizes：

### Separate Passes

由于轴对齐的2D高斯在数学上就是可拆的：

$$
G_{2D}(x,y)=G_{1D}(x)·G_{1D}(y)
$$

所以可以把一个$N*N$的2D高斯，水平方向做一次$1*N$，垂直方向做一次$N*1$，这使得$N*N$的复杂度降到了$2N$。于是可以把公式做如下变化，让一部分式子先积起来：

$$
\iint F(x_0,y_0)G_{2D}(x_0-x,y_0-y)dxdy = \int \left( \int F(x_0,y_0)G_{1D}(x_0-x)\right) G_{1D}(y_0-y)dy
$$

理论上，从双边滤波开始很多复杂的滤波无法使用这种简单的拆分。但是实践上，可以强行拆成横着一遍竖着一遍，只要范围不太大，$32×32$的大小看不出什么明显瑕疵。

### Progressively Growing Sizes

> 核心理念：逐渐增大核的大小和样本的间隔，每次实际核的大小都比较小

有个专门的术语叫做 à-trous wavelet

比如每一趟的核都是$5×5$的大小，核的尺寸不变。但是核元素之间会有空隙，即每个元素之间可能隔着0个到n个元素。下图展示了一个$3×3$滤波核元素间隔由0个变为1个的情况：

```
                [a 0 b 0 c]
[a b c]         [0 0 0 0 0]
[d e f]    =>   [d 0 e 0 f]
[g h i]         [0 0 0 0 0]
                [g 0 h 0 i]
```

相当于对于要处理的中心元素，隔$2^i-1$的步长取元素，其中$i$从0开始增加

此时滤波核的大小感受野（receptive field），即卷积操作能够“看到”的输入图像区域的大小得到了增大，从而实现了在不同尺度上提取图像的特征。

---

> 关于其数字信号原理，可查阅 games101 Raster2 50min处内容

<mark>为什么 filter size 是逐步增加的</mark>：在图像滤波中，更大的滤波器尺寸通常意味着移除更低的频率。

我查了一下，他这个说的似乎不绝对，只能说有些滤波符合这个。反例就是对于低通滤波器导致图像模糊，显然是去除高频。而 à-trous wavelet 滤波既包含低通滤波也包含高通滤波，是一种多尺度分解方法，当其滤波的尺寸变大时，滤波器的行为会更加偏向低通滤波。

<mark>为什么可以跳过很多采样样本</mark>：采样的本质是在频率上搬移/重复频谱，采样的密集则频谱搬运的距离大。

采样过程在时域上是对信号进行离散化，而在频域上则会导致信号的频谱<u>周期性复制</u>（即频谱搬移）

但是当频谱搬移时，如果目标位置已经存在频率成分，就会发生频谱重叠或混叠（aliasing），这会导致信号失真或信息丢失。

![滤波与频谱关系示意](D:\CGNote\Games202\img\lecture13_2.png)

<u>理想状态下</u>低通滤波会去除掉更高频率的信息（上左图蓝色区域），第二次 à-trous wavelet 时则留下更加低频的信息（去除掉黄色区域）。然后在第二次滤波的时候发生的采样，实际上就是把上右图蓝色区域搬运到其右边界挨着的的黄色区域。

如此多遍之后相当于把一些低频的波段反复重复。如果直接用大尺寸的滤波，会导致搬移的时候目标位置的频率尚未清除。逐步增大的尺寸可以避免 aliasing。

> 注：AI 说à-trous wavelet 滤波没有进行下采样，因此不会导致频谱搬移。我怀疑 AI 的意思是只做了移除，没做频谱搬移，此处待定。

由于 à-trous wavelet 滤波并非绝对的高通或者低通，这导致实际上在做滤波的时候会留下一些高频的信息，就结果而言则是产生格子状的 artifact。

## Outlier Removal

蒙特卡洛方法采样得到的渲染结果中可能会有很多亮点，即便他们显示的是类似于白色，实际上亮度值可能大得多，在这种情况下应用滤波，虽然颜色值被滤波降下来了，但是依旧超过1，显示效果上亮点依旧存在，甚至可能会导致其值扩散至周围，形成一块亮的区域。

这些超级亮的像素称之为 outlier/ firefly，处理这类瑕疵的手法叫做 Outlier Removal。

对它的处理会导致能量不守恒。

Outlier Detection：

探测周围一定范围内的区块（例如$7*7$大小）并计算均值和方差，位于$k$个标准差之外的认为是 Outlier，即$[\mu - k\sigma,\mu + k \sigma]$内认为是合理的。

Outlier Removal：

把超过这个范围的值 clamp 到这个范围（而非简单地移除）。工业界做法可能更加复杂，会在其他颜色空间处理

---

如果场景中有些很小的光源，Outlier Removal 是否会将之消除？

会消除，但是可以通过手段避免：可以先不渲染光源，等做完 Outlier Removal 之后再把光源渲染上去。

---

在上文提到了在做 Temporal Denoising 的时候，可能因为Motion Vector 没有命中造成鬼影（ghosting）现象，然后在补救方案中提到了可以 clamp 到本帧像素位置邻域一定标准差范围内，也是类似的思想。

## 具体方法

- SVGF

- RVE

# Reference

[1] [Ray Tracing Denoising](https://alain.xyz/blog/ray-tracing-denoising#sampling)

[2] [Motion vectors in URP](https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@16.0/manual/features/motion-vectors.html)

[3] [Per-Object Motion Blur](https://john-chapman-graphics.blogspot.com/2013/01/per-object-motion-blur.html)
