# 前情提要

先前的 lecture 提到了 Real-Time Physically-Based Materials

- Microfacet BRDF, NDF, shadowing-masking（但是依旧只能处理直接光照）

- Kulla-Conty Approximation for multiple bounces

- Disney principled BRDF（Artist Friendly）

提到了使用 Linearly Transformed Cosines (LTCs) 对 polygonal lighting进行高效的近似

提到了为什么202不讲Volumetric/scattering materials：因为他们依赖太多前置的知识了，例如RTE、BSSRDF、single/multiple sacttering等

实时光追的突破源自于硬件的突破

# Basic of Denoising

## 1 sample per pixel：Cause of Noise

简称1 SPP，1 SPP意味着：

- 1 rasterization（即相机到像素的primary ray）

- 1 ray（像素到光源的visibility）

- 1 ray（secondary bounce）

- 1 ray（光线弹射一次后的 可见性，secondary visibility）

前两个加起来是直接光照。相机到像素本质是光栅化，所以用rasterization代替了ray

现在的硬件支持实时 SPP 的次数不多，因此只讨论最基础的1 SPP的情况。但是1 SPP 的效果非常noisy，所以<mark>RTRT的核心就是Denoising</mark>。

在1 SPP下denoising的目标是这样的：

1. 质量（no overblur，no artifacts，keep all details）

2. Speed（<2ms to denoise one frame）

overblur：过于夸张的Filter会去掉噪点，也会去掉有用的信息

artifacts：渲染过程中出现的可见的瑕疵

这样的要求导致原先的滤波手段不太符合要求，例如：

- Sheared filtering series（SF, AAF, FSF, MAAF）

- Other offline filtering methods（IPP, BM3D, APR）

- Deep learning series（CNN, Autoencoder）

降噪的方法很多，但是针对RTRT的很少。或多或少在采样或者耗时上都不满足。

---

工业界的做法的核心思想是使用Temporal Denoising，在时间上处理：

1. 在滤波当前帧的情况下永远认为前一帧是已经滤波好了的

2. 使用motion vector寻找shading point在上一帧的位置

3. 复用上一帧的降噪信息

这是一种Inspired by TAA的方法

在这种方法中，通常需要 G-Buffer 辅助

geometry buffer 能够记录渲染过程中可以获得的信息，这些信息通常是 screen space 的，内容与片元密切相关。其内容通常源自渲染过程中用到或者产生的数据，例如片元的世界坐标、pixel depth

## Motion Vector: The Basis of Denoising

> 这块的东西我确信 202 讲的不够清晰

首先 Velocity Buffer$ ^{[1]}$ 和 Motion Vector Buffer 本质上是同一个概念，它们的作用都是存储每个像素或顶点在连续帧之间的运动信息（即运动向量，motion vector）。

使用 motion vector 的核心目的是为了追踪世界坐标在屏幕空间中的运动，而不是直接与某个具体的物体绑定。它的本质是通过世界坐标的连续性，将当前帧的像素与上一帧的像素关联起来，从而利用时间域的信息（即上一帧的降噪结果）来改善当前帧的质量。

Motion Vector是一个屏幕空间的二维向量，定义如下：

```glsl
motion_vector = current_screen_pos.xy - prev_screen_pos.xy
```

但是这里有个问题：像素展示的是场景中的物体的某一块区域$\Delta$的颜色，这块区域有个世界空间坐标$s$。那么前一帧如果物体发生移动，导致$\Delta$并不在$s$位置，此时该取用谁来确定前一帧像素的屏幕空间位置？因此要说明 Motion Vector 的种类。

Motion Vector 分为两种：Camera Motion Vector 和 Object Motion Vector

仅给定运动矢量纹理无法推断$^{[2]}$片元在屏幕上的运动是否仅由相机运动、物体运动还是两者的结合引起。

- Camera Motion Vector 的计算与场景复杂度无关，仅需两帧的相机矩阵即可。

- Object Motion Vector 的计算与场景复杂度有关，需要考虑每个物体的运动，同时也需要应用相机的运动贡献

---

根据相关信息推导出 Motion Vector 的操作叫做 Back Projection。

> 核心是确定当前帧的像素在上一帧的哪里

在这个计算的过程中，究竟计算的是上面的提到的两种 Motion Vector 中的哪一种？据观察网络博客$^{[3]}$所提供的思路，相当于是两个都计算了。本身理应是针对每个片元的计算操作，但是由于顶点着色器数据传到片元着色器会进行插值的性质，因此在顶点着色器 foreach vertex 的本质实际上就是针对每个要渲染的物体的操作，因此也把 Object Motion Vector 计算进去了。

但是1小时16分那里讲Temporal Failure的时候提到的，相机没移动但是场景物体移动了，此时他认为$s = s'$，这相当于算的是 Camera Motion Vector。

具体计算思路：

1. 确定当前像素的世界坐标$s$：一般情况下G-Buffer里会有记录片元的世界坐标，没有的话只能手动计算
2. 确定上一帧的世界坐标$s'$：如果没发生运动那$s=s'$，否则由$s'\overset{T}\rightarrow s$得到$s\overset{T^{-1}}\rightarrow s'$
3. 根据$s'$确认投影到屏幕空间的坐标$x'$（当前片元在屏幕空间的坐标是$x$）
4. $x-x'$即为 motion vector

---

闫老师课上说这样逆推世界坐标，但是我疑惑的是应该不用乘$M^{-1}$，那样就是模型坐标了。我怀疑他说的s是模型坐标不是世界坐标。

$$
s = M^{-1}V^{-1}P^{-1}E^{-1}x
$$

其中

- x是屏幕空间坐标（二维）加上z值作为z通道

- $E$是相机外参矩阵$V$的逆矩阵，用于视口变换，不过OpenGL中视口变换是自动完成的

---

不难发现确实会出现一些极端情况，导致本帧某些片元无法得到motion vector。比如说相同的世界坐标，这一帧在物体A上，但是上一帧物体 A 还没出现，该世界坐标处没有任何物体；或是说上一帧的世界坐标到相机之间有其它遮挡物。这些情况下无法直接使用时间域的去噪手段。这里的细节可以看后文章节。

---

CV中有一种手法叫做 Optical Flow（光流），Motion Vector类似于一种光流，但是比他更准确。在 Ray Tracing 里使用光流的效果和速度也并不是很符合预期。

## Temporal Accum./Denoising

有了一个片元在两帧上的信息，接下来就是利用前一帧。最简单的方式就是线性混合

做如下定义：

- $\sim$：unfiltered 未filter的，噪声较多的内容

- $-$：filtered 没有噪声或者噪声比较小的

那么在空间上的降噪公式是：

$$
\bar{C}^{(i)}=Filter[\bar{C}^{(i)}]
$$

在时间上的降噪公式是：

$$
\bar{c}^i = \alpha\bar{C}^{(i)}+(1-\alpha)\bar{C}^{(i-1)}
$$

其中$\alpha$表示平衡系数，即当前帧的贡献，一般$\alpha \in [0.1,0.2]$，这意味着上一帧通常要贡献80%~90%

要点：

- 所有降噪/滤波要保持能量守恒，并不应该使画面变亮/暗

<mark>缺陷</mark>：

- 降噪滤波可能导致AO缺失（尤指contact hardening/contact shadow）

## Temporal Failure

有一些典型的，不能够使用上一帧信息的情况;

<mark>Temporal Info is not always visible</mark>，其典型情况如下：

- burn-in period：突然切换场景（需要burn-in period来累计新场景的几帧信息以供后续复用）

- screen space issue：Zoom out（倒退着走）

- disocclusion：相机移动导致的物体突然间不被遮挡

<mark>Shading Error</mark>：有些时候 Temporal Failure 源自于 Shading 出了问题。

- 光照突然发生巨大变化
- 场景完全没动（Motion Vector为0）但是光源动了，导致 detached shadow 问题

<mark>Glossy Reflection Error</mark>：

- 宏观表现是反射的影子的变化和物体实际变化有一点时间的延迟
- 传统 Motion Vector 表示的是物体的变化，无法捕捉光影的变化

## Adjustments to Temporal Failure

- 强行用上一帧的信息：拖尾（lagging）

- 有选择地用上一帧的信息
  
  - Clamping：对本帧周围N*N计算均值$\mu$和方差$σ²$，上一帧的颜色先clamp到$\mu \pm a ·\sigma^2$，其中$a \in [1,3]$
  
  - Detection：检测要不要用前一帧的信息，如果 Motion Vector 要到另一个物体上找信息（利用 ID Map）那肯定不太适合用前一帧的。

针对检测到不能用：

- $\alpha$根据Temporal Failure的程度调节一下，可以极端到完全不用上一帧的值

- 更相信当前帧意味着<mark>重新引入一些噪声</mark>

# 滤波

## Spectial Denoising

如何对本帧的信息进行滤波：Cross / joint bilateral Filter 及其变体

噪声在高频或者低频都有可能出现，有用的信息也同理。高通滤波/低通滤波只能得到一个相对可以接受的结果，并且伴有有用的信息的丢失。

> 图像某点颜色本身不代表频率，颜色变化表示频率

关于图像的高低频：

- 高频分量对应于图像中快速变化的区域，例如边缘、纹理和细节

- 低频分量对应于图像中变化缓慢的区域，例如平滑的背景和大的色块

## Gaussian Filter

高斯滤波是因为使用高斯函数作为卷积核而得名。

$$
G(x, y) = A \cdot e^{-\frac{x^2 + y^2}{2\sigma^2}}
$$

二维高斯函数是二维正态的概率密度函数乘以一个系数，在系数不为1时相当于是未归一化的二维正态的概率密度函数，并且一般也不需要是归一化的。归一化的意义是在整个二维空间上积分为1。

用高斯函数生成采样核的操作叫做<mark>离散化高斯核</mark>，这个操作的本质是以高斯函数中心为中心，以一定的步长采样 N x N 的点。

步长和标准差没有直接关系，一般步长设置为1，而标准差$\sigma$决定核的大小，一般情况下是$(6σ + 1) × (6σ + 1)$的大小

<mark>高斯滤波的特征</mark>：

- 是低通滤波

- 线性滤波

- 可分离性：可分解为两个1D高斯滤波

## Bilateral filtering

双边滤波对范围内的每个像素加权平均，其权重同时基于像素的欧几里得距离和辐射差异（例如，颜色强度、深度距离等）

特点：保持边缘；非线性

## Implementing Large Filters

当滤波器的滤波核大小过大的时候，例如64*64，就会导致滤波过程显著变慢。对此工业界一般有两种做法

- Separate Passes：拆分实现

- Progressively Growing Sizes：

### Separate Passes

由于2D高斯在数学上就是可拆的：

$$
G_{2D}(x,y)=G_{1D}(x)·G_{1D}(y)
$$

所以可以把一个$N*N$的2D高斯，水平方向做一次$1*N$，垂直方向做一次$N*1$

# Reference

[1] [Ray Tracing Denoising](https://alain.xyz/blog/ray-tracing-denoising#sampling)

[2] [Motion vectors in URP](https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@16.0/manual/features/motion-vectors.html)

[3] [Per-Object Motion Blur](https://john-chapman-graphics.blogspot.com/2013/01/per-object-motion-blur.html)
